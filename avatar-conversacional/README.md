# Avatar Conversacional 3D en Tiempo Real

Una aplicaci√≥n web de producci√≥n que permite conversar por voz con un asistente de IA a trav√©s de un avatar 3D con sincronizaci√≥n labial y gestos faciales. Optimizada para escritorio y dispositivos m√≥viles modernos.

## Caracter√≠sticas principales

- **Conversaci√≥n por voz en tiempo real** con latencia < 400ms
- **Avatar 3D con sincronizaci√≥n labial** usando Ready Player Me y morph targets ARKit
- **Dos modos de operaci√≥n**:
  - **Modo A**: Baja latencia con OpenAI Realtime API y lip-sync heur√≠stico
  - **Modo B**: Lip-sync profesional con Deepgram + Azure Speech TTS + visemes
- **Detecci√≥n autom√°tica de turnos (VAD)** con WebAudio API
- **Barge-in support** - Interrumpe al asistente hablando
- **Subt√≠tulos en vivo** y transcripci√≥n completa
- **Accesibilidad** con controles de velocidad, volumen y sensibilidad

## Tecnolog√≠as

- **Frontend**: Next.js 14, TypeScript, React 18, TailwindCSS
- **3D**: React Three Fiber, drei, Three.js
- **Avatar**: Ready Player Me (GLTF/GLB con morph targets)
- **Audio**: WebRTC, WebAudio API
- **IA/Voz**:
  - OpenAI Realtime API (conversaci√≥n)
  - Deepgram Live (STT en Modo B)
  - Azure Speech TTS (TTS con visemes en Modo B)
- **State**: Zustand

## Instalaci√≥n

### Requisitos previos

- Node.js 18+ (recomendado 20+)
- pnpm 8+
- Claves API (opcional, funciona en modo mock sin ellas):
  - OpenAI API Key
  - Deepgram API Key (solo Modo B)
  - Azure Speech Services Key y Region (solo Modo B)

### Pasos

1. **Clonar el repositorio**

```bash
git clone <tu-repo>
cd avatar-conversacional
```

2. **Instalar dependencias**

```bash
pnpm install
```

3. **Configurar variables de entorno**

Copia el archivo de ejemplo y configura tus claves:

```bash
cp .env.example .env.local
```

Edita `.env.local`:

```env
# OpenAI Realtime API (obligatorio para Modo A)
OPENAI_API_KEY=tu_clave_aqui
OPENAI_REALTIME_MODEL=gpt-4o-realtime-preview

# Deepgram (solo para Modo B - STT)
DEEPGRAM_API_KEY=tu_clave_aqui

# Azure Speech (solo para Modo B - TTS con visemes)
AZURE_SPEECH_KEY=tu_clave_aqui
AZURE_SPEECH_REGION=tu_region_aqui  # ej: eastus

# Configuraci√≥n (opcional)
NEXT_PUBLIC_MODE=A  # A o B
NEXT_PUBLIC_ENABLE_LOGGING=true
```

4. **Descargar avatar 3D**

Descarga un avatar de Ready Player Me y col√≥calo en `public/models/avatar.glb`:

```bash
# Opci√≥n 1: Crear tu propio avatar
# Visita https://readyplayer.me/ y crea un avatar
# Descarga el archivo .glb

# Opci√≥n 2: Usar avatar de ejemplo
# Descarga desde: https://models.readyplayer.me/[avatar-id].glb
# Gu√°rdalo como public/models/avatar.glb
```

Si no tienes un avatar, la aplicaci√≥n mostrar√° un error en la consola pero seguir√° funcionando en otros aspectos.

5. **Ejecutar en desarrollo**

```bash
pnpm dev
```

Abre [http://localhost:3000](http://localhost:3000) en tu navegador.

### üê≥ Ejecuci√≥n con Docker (Windows)

Si prefieres usar Docker en Windows, puedes ejecutar la aplicaci√≥n de forma aislada y con todos los requisitos preconfigurados.

#### Requisitos

1. **Docker Desktop** para Windows
   - Descarga: https://www.docker.com/products/docker-desktop/
   - Aseg√∫rate de que est√© corriendo

#### Uso R√°pido

**Opci√≥n 1: Script de PowerShell (Recomendado)**
```powershell
# Ejecutar en modo producci√≥n
.\start-avatar-docker.ps1

# Ejecutar en modo desarrollo
.\start-avatar-docker.ps1 -Dev

# Detener contenedores
.\start-avatar-docker.ps1 -Stop

# Ver logs
.\start-avatar-docker.ps1 -Logs
```

**Opci√≥n 2: Script Batch (CMD/Git Bash)**
```cmd
REM Modo producci√≥n
start-avatar-docker.bat

REM Modo desarrollo
start-avatar-docker.bat dev

REM Detener
start-avatar-docker.bat stop

REM Ver logs
start-avatar-docker.bat logs
```

**Opci√≥n 3: Comandos Directos**
```bash
# Modo producci√≥n
docker-compose up -d --build

# Modo desarrollo (con hot-reload)
docker-compose up -d --build avatar-dev

# Detener
docker-compose down

# Ver logs
docker-compose logs -f
```

#### URLs de Acceso

- **Producci√≥n**: http://localhost:3000
- **Desarrollo**: http://localhost:3000 (con hot-reload)

#### Comandos √ötiles

```bash
# Ver estado de contenedores
docker-compose ps

# Ver logs en tiempo real
docker-compose logs -f --tail=50

# Parar contenedores
docker-compose down

# Rebuild completo
docker-compose build --no-cache
```

#### Soluci√≥n de Problemas

**Puerto ocupado:**
```bash
# Ver qu√© usa el puerto 3000
netstat -ano | findstr :3000

# Usar otro puerto
docker-compose up -d -p 3001:3000
```

**Ver documentaci√≥n completa:** [DOCKER_WINDOWS.md](./DOCKER_WINDOWS.md)

## Uso

### Modo A - Baja latencia (OpenAI Realtime)

1. Navega a `/avatar`
2. Aseg√∫rate de que "Modo A" est√© seleccionado
3. Haz clic en "Conectar"
4. Permite acceso al micr√≥fono
5. Habla naturalmente - el sistema detectar√° tu voz autom√°ticamente
6. El avatar responder√° con sincronizaci√≥n labial aproximada basada en energ√≠a de audio

**Caracter√≠sticas del Modo A**:
- Latencia ultra-baja (< 250ms)
- Conexi√≥n WebRTC directa con OpenAI
- Lip-sync heur√≠stico (jawOpen, mouthFunnel, mouthPucker)
- Ideal para conversaciones fluidas

### Modo B - Lip-sync profesional

1. Navega a `/settings` y selecciona "Modo B"
2. Regresa a `/avatar` y haz clic en "Conectar"
3. El sistema usar√°:
   - Deepgram para transcripci√≥n (STT)
   - OpenAI para generaci√≥n de respuesta (LLM)
   - Azure Speech para s√≠ntesis con visemes (TTS)
4. La sincronizaci√≥n labial ser√° precisa frame-by-frame

**Caracter√≠sticas del Modo B**:
- Lip-sync preciso con visemes Azure/ARKit
- Mapeo completo de visemes a blendshapes
- Mayor latencia (~400-600ms) pero mejor calidad visual
- Ideal para demos, presentaciones, tutoriales

### Controles

- **Conectar/Desconectar**: Inicia o termina la sesi√≥n
- **Silenciar**: Desactiva tu micr√≥fono temporalmente
- **Push to Talk**: Mant√©n presionada la barra espaciadora para hablar
- **Selector de voz**: Elige entre 6 voces de OpenAI
- **Sensibilidad VAD**: Ajusta qu√© tan sensible es la detecci√≥n de voz

## Arquitectura

### Estructura del proyecto

```
avatar-conversacional/
‚îú‚îÄ‚îÄ app/
‚îÇ   ‚îú‚îÄ‚îÄ api/
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ realtime/route.ts      # Endpoint para OpenAI Realtime
‚îÇ   ‚îÇ   ‚îú‚îÄ‚îÄ deepgram/route.ts      # Proxy para Deepgram STT
‚îÇ   ‚îÇ   ‚îî‚îÄ‚îÄ azure-tts/route.ts     # Azure TTS con visemes
‚îÇ   ‚îú‚îÄ‚îÄ avatar/page.tsx             # P√°gina principal de conversaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ settings/page.tsx           # Configuraci√≥n
‚îÇ   ‚îî‚îÄ‚îÄ page.tsx                    # Landing page
‚îú‚îÄ‚îÄ components/
‚îÇ   ‚îú‚îÄ‚îÄ AvatarCanvas.tsx            # Avatar 3D con React Three Fiber
‚îÇ   ‚îú‚îÄ‚îÄ Controls.tsx                # UI de controles
‚îÇ   ‚îî‚îÄ‚îÄ Captions.tsx                # Subt√≠tulos y transcripci√≥n
‚îú‚îÄ‚îÄ lib/
‚îÇ   ‚îú‚îÄ‚îÄ state.ts                    # Zustand store
‚îÇ   ‚îú‚îÄ‚îÄ audio.ts                    # Captura de micr√≥fono y VAD
‚îÇ   ‚îú‚îÄ‚îÄ webrtc.ts                   # Cliente WebRTC para Realtime API
‚îÇ   ‚îú‚îÄ‚îÄ viseme-mapper.ts            # Mapeo viseme‚ÜíARKit blendshapes
‚îÇ   ‚îî‚îÄ‚îÄ lip-sync-energy.ts          # Lip-sync heur√≠stico (Modo A)
‚îî‚îÄ‚îÄ public/
    ‚îî‚îÄ‚îÄ models/
        ‚îî‚îÄ‚îÄ avatar.glb              # Modelo 3D del avatar
```

### Flujo de datos

#### Modo A (Baja latencia)

```
Usuario (micr√≥fono)
  ‚Üì
AudioCapture (VAD)
  ‚Üì
OpenAI Realtime API (WebRTC)
  ‚Üì (audio + transcripci√≥n)
LipSyncEnergy (an√°lisis de energ√≠a)
  ‚Üì
AvatarCanvas (morph targets)
```

#### Modo B (Lip-sync Pro)

```
Usuario (micr√≥fono)
  ‚Üì
AudioCapture (VAD)
  ‚Üì
Deepgram Live (STT WebSocket)
  ‚Üì (transcripci√≥n)
OpenAI API (LLM)
  ‚Üì (texto respuesta)
Azure Speech TTS (con visemes)
  ‚Üì (audio + visemes)
VisemeMapper
  ‚Üì (ARKit blendshapes)
AvatarCanvas (morph targets)
```

### Componentes clave

#### AudioCapture

- Captura audio del micr√≥fono con WebAudio API
- Implementa VAD (Voice Activity Detection) con AnalyserNode
- Configuraci√≥n de echo cancellation, noise suppression
- Eventos: `onSpeechStart`, `onSpeechEnd`, `onAudioLevel`

#### OpenAIRealtimeClient

- Conexi√≥n WebRTC con OpenAI Realtime API
- Manejo de data channels para eventos
- Streaming de audio bidireccional
- Soporte para barge-in (interrupciones)

#### VisemeMapper

- Mapeo de visemes Azure (0-21 + sil) a blendshapes ARKit
- Interpolaci√≥n suave entre estados
- Smoothing temporal para evitar jitter

#### LipSyncEnergy

- An√°lisis de energ√≠a RMS y crest factor
- Heur√≠stica para calcular jawOpen, mouthFunnel, mouthPucker
- Attack/release configurable (30ms/80ms)

## M√©tricas de latencia

### Objetivo (Modo A)

- **Captura ‚Üí Transcripci√≥n parcial**: ‚â§ 250ms
- **Audio de respuesta streaming**: < 400ms tras fin de turno
- **Latencia total E2E**: < 600ms

### Medici√≥n

Activa logging con `NEXT_PUBLIC_ENABLE_LOGGING=true` en `.env.local`. Las m√©tricas aparecer√°n en la UI de subt√≠tulos.

## Despliegue

### Vercel (recomendado)

1. Push tu c√≥digo a GitHub
2. Importa el proyecto en Vercel
3. Configura variables de entorno en Vercel:
   - `OPENAI_API_KEY`
   - `DEEPGRAM_API_KEY` (opcional)
   - `AZURE_SPEECH_KEY` (opcional)
   - `AZURE_SPEECH_REGION` (opcional)
4. Deploy

```bash
pnpm build
vercel --prod
```

### Otros proveedores

El proyecto es compatible con cualquier proveedor que soporte Next.js:
- Netlify
- Railway
- Render
- Cloudflare Pages

### TURN Server (opcional)

Para redes con NAT/CGNAT restrictivo, configura un servidor TURN:

```bash
# Usando coturn con Docker
docker run -d --network=host \
  -e DETECT_EXTERNAL_IP=yes \
  coturn/coturn \
  -n --log-file=stdout \
  --min-port=49152 --max-port=65535 \
  --lt-cred-mech \
  --user=usuario:contrase√±a \
  --realm=tu-dominio.com
```

Luego configura en `.env.local`:

```env
TURN_URI=turn:tu-servidor.com:3478
TURN_USER=usuario
TURN_PASS=contrase√±a
```

## Tests

### Tests manuales

1. **Test de conexi√≥n**
   - [ ] El bot√≥n "Conectar" solicita permisos de micr√≥fono
   - [ ] La conexi√≥n se establece en < 3 segundos
   - [ ] El indicador de estado muestra "Conectado"

2. **Test de VAD**
   - [ ] El nivel de audio responde a tu voz
   - [ ] La detecci√≥n de habla inicia/termina correctamente
   - [ ] No hay falsos positivos con ruido de fondo moderado

3. **Test de conversaci√≥n**
   - [ ] La transcripci√≥n aparece en subt√≠tulos en vivo
   - [ ] El asistente responde en < 1 segundo
   - [ ] El audio del asistente se reproduce claramente

4. **Test de lip-sync**
   - **Modo A**: Los labios se mueven sincronizados con la energ√≠a del audio
   - **Modo B**: Los labios forman visemes espec√≠ficos para cada fonema

5. **Test de barge-in**
   - [ ] Hablar mientras el asistente habla lo interrumpe
   - [ ] El audio del asistente se detiene inmediatamente
   - [ ] El sistema est√° listo para recibir tu entrada

6. **Test de accesibilidad**
   - [ ] Los subt√≠tulos son legibles y precisos
   - [ ] Los controles funcionan con teclado (Tab, Enter, Espacio)
   - [ ] Funciona en modo de alto contraste

### Script de medici√≥n de latencia

```typescript
// En lib/latency-logger.ts
export class LatencyLogger {
  private timestamps: { [key: string]: number } = {};

  mark(event: string) {
    this.timestamps[event] = performance.now();
  }

  measure(from: string, to: string): number {
    return this.timestamps[to] - this.timestamps[from];
  }

  report() {
    console.table({
      'Captura ‚Üí STT': this.measure('capture', 'transcription'),
      'STT ‚Üí LLM': this.measure('transcription', 'llm_response'),
      'LLM ‚Üí TTS': this.measure('llm_response', 'audio_start'),
      'Total E2E': this.measure('capture', 'audio_start'),
    });
  }
}
```

## Troubleshooting

### El micr√≥fono no funciona

- **Chrome/Edge**: Aseg√∫rate de usar HTTPS (o localhost)
- **Safari**: Verifica permisos en Preferencias del Sistema
- **Firefox**: Revisa `about:permissions` para el sitio

### Audio entrecortado o con eco

- Activa echo cancellation en `lib/audio.ts`
- Usa auriculares en lugar de altavoces
- Reduce la sensibilidad VAD en Settings

### El avatar no aparece

- Verifica que `public/models/avatar.glb` existe
- Aseg√∫rate de que el modelo tiene morph targets
- Revisa la consola del navegador para errores de Three.js

### Alta latencia

- **Modo A**: Verifica tu conexi√≥n a internet (ping a api.openai.com)
- **Modo B**: Considera cambiar a Modo A si no necesitas lip-sync preciso
- Configura un servidor TURN m√°s cercano geogr√°ficamente

### Error "API Key not configured"

- Copia `.env.example` a `.env.local`
- Configura las claves API necesarias
- Reinicia el servidor de desarrollo (`pnpm dev`)

## Modo Mock (sin APIs)

Si no tienes claves API, la aplicaci√≥n funciona en modo mock:

- **Realtime API**: Respuestas simuladas tras 2 segundos
- **Transcripci√≥n**: Texto hardcodeado
- **TTS**: Sin audio real, pero visemes generados
- **Lip-sync**: Movimientos aleatorios del avatar

Esto es √∫til para:
- Desarrollo de UI sin consumir cr√©ditos
- Testing de la l√≥gica de sincronizaci√≥n
- Demos sin conexi√≥n

## Limitaciones conocidas

- **Safari iOS**: WebRTC puede requerir interacci√≥n del usuario para iniciar
- **Modo B**: Latencia mayor (400-600ms) vs Modo A (< 250ms)
- **Avatar personalizado**: Requiere descarga manual de Ready Player Me
- **Idiomas**: Optimizado para espa√±ol, pero soporta otros idiomas configurando la voz

## Extras opcionales (roadmap)

- [ ] Selector de avatar desde URL de Ready Player Me
- [ ] An√°lisis de sentimiento para expresiones faciales
- [ ] Editor de system prompt en UI
- [ ] Exportar conversaci√≥n como SRT/VTT
- [ ] Soporte para m√∫ltiples idiomas con detecci√≥n autom√°tica
- [ ] Modo oscuro mejorado
- [ ] Animaciones de idle m√°s complejas (respiraci√≥n, micro-gestos)

## Contribuciones

Las contribuciones son bienvenidas. Por favor:

1. Fork el repositorio
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request

## Licencia

MIT License - ver el archivo LICENSE para m√°s detalles.

## Agradecimientos

- [OpenAI](https://openai.com/) por la Realtime API
- [Ready Player Me](https://readyplayer.me/) por los avatares
- [React Three Fiber](https://docs.pmnd.rs/react-three-fiber/) por el rendering 3D
- [Deepgram](https://deepgram.com/) por el STT de baja latencia
- [Microsoft Azure](https://azure.microsoft.com/es-es/services/cognitive-services/speech-services/) por los servicios de voz

## Soporte

Para preguntas o issues, abre un issue en GitHub o contacta al maintainer.

---

Hecho con ‚ù§Ô∏è usando Next.js, React Three Fiber y OpenAI Realtime API
